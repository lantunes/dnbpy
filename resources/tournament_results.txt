Results:

board size: 2x2
player 1 (RandomPolicy) vs. player 2 (Level1HeuristicPolicy)
RandomPolicy: {'lost': 7761, 'tied': 1799, 'won': 440}; Level1HeuristicPolicy: {'lost': 440, 'tied': 1799, 'won': 7761}

board size: 2x2
player 1 (RandomPolicy) vs. player 2 (Level2HeuristicPolicy)
RandomPolicy: {'won': 312, 'tied': 1693, 'lost': 7995}; Level2HeuristicPolicy: {'won': 7995, 'tied': 1693, 'lost': 312}

board size: 2x2
player 1 (Level1HeuristicPolicy) vs. player 2 (Level2HeuristicPolicy)
Level1HeuristicPolicy: {'lost': 5408, 'tied': 3361, 'won': 1231}; Level2HeuristicPolicy: {'lost': 1231, 'tied': 3361, 'won': 5408}

board size: 3x3
player 1 (RandomPolicy) vs. player 2 (Level1HeuristicPolicy)
RandomPolicy: {'tied': 0, 'won': 31, 'lost': 9969}; Level1HeuristicPolicy: {'tied': 0, 'won': 9969, 'lost': 31}

board size: 3x3
player 1 (RandomPolicy) vs. player 2 (Level2HeuristicPolicy)
RandomPolicy: {'lost': 9966, 'won': 34, 'tied': 0}; Level2HeuristicPolicy: {'lost': 34, 'won': 9966, 'tied': 0}

board size: 3x3
player 1 (Level1HeuristicPolicy) vs. player 2 (Level2HeuristicPolicy)
Level1HeuristicPolicy: {'lost': 8409, 'won': 1591, 'tied': 0}; Level2HeuristicPolicy: {'lost': 1591, 'won': 8409, 'tied': 0}

board size: 5x5
player 1 (RandomPolicy) vs. player 2 (Level1HeuristicPolicy)
RandomPolicy: {'lost': 10000, 'won': 0, 'tied': 0}; Level1HeuristicPolicy: {'lost': 0, 'won': 10000, 'tied': 0}

board size: 5x5
player 1 (RandomPolicy) vs. player 2 (Level2HeuristicPolicy)
RandomPolicy: {'lost': 10000, 'tied': 0, 'won': 0}; Level2HeuristicPolicy: {'lost': 0, 'tied': 0, 'won': 10000}

board size: 5x5
player 1 (Level1HeuristicPolicy) vs. player 2 (Level2HeuristicPolicy)
Level1HeuristicPolicy: {'tied': 0, 'won': 461, 'lost': 9539}; Level2HeuristicPolicy: {'tied': 0, 'won': 9539, 'lost': 461}


    ----------------------------------------------------------------------------------------
            | L0 2x2 | L1 2x2 | L2 2x2 | L0 3x3 | L1 3x3 | L2 3x3 | L0 5x5 | L1 5x5 | L2 5x5
    ----------------------------------------------------------------------------------------
     L0 2x2 |   -    |  4.40% |  3.12% |   -    |   -    |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L1 2x2 | 77.61% |    -   | 12.31% |   -    |   -    |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L2 2x2 | 79.95% | 54.08% |    -   |   -    |   -    |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L0 3x3 |   -    |    -   |    -   |   -    | 0.31%  | 0.34%  |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L1 3x3 |   -    |    -   |    -   | 99.69% |   -    | 15.91% |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L2 3x3 |   -    |    -   |    -   | 99.66% | 84.09% |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L0 5x5 |   -    |    -   |    -   |   -    |   -    |   -    |    -   |   0%   |   0%
    ----------------------------------------------------------------------------------------
     L1 5x5 |   -    |    -   |    -   |   -    |   -    |   -    |  100%  |   -    | 4.61%
    ----------------------------------------------------------------------------------------
     L2 5x5 |   -    |    -   |    -   |   -    |   -    |   -    |  100%  | 95.39% |   -
    ----------------------------------------------------------------------------------------
    Win rates of L0, L1, and L2 policies versus each other on 2x2, 3x3, and 5x5,
     after tournaments consisting of 10,000 games each.
    L0: A policy that selects an edge at random.
    L1: A policy that selects an edge randomly that completes a box, if possible. Otherwise,
        it selects an edge at random.
    L2: A policy that selects an edge randomly that completes a box, if possible. Otherwise,
        it selects an edge randomly that does not allow the opponent to complete a box.
        Otherwise, it selects an edge at random.

board size: 2x2
CausalEntropicPolicy(max_sample_paths=1000) vs RandomPolicy:
CausalEntropicPolicy: {'won': 364, 'lost': 21, 'tied': 115} -> 72.8%
CausalEntropicPolicy(max_sample_paths=10000) vs RandomPolicy:
CausalEntropicPolicy: {'won': 69, 'lost': 1, 'tied': 30} -> 69%

board size: 2x2
CausalEntropicPolicy(max_sample_paths=1000) vs Level1HeuristicPolicy:
CausalEntropicPolicy: {'won': 5, 'lost': 371, 'tied': 124} -> 0.01%
CausalEntropicPolicy(max_sample_paths=10000) vs Level1HeuristicPolicy:
CausalEntropicPolicy: {'won': 4, 'lost': 84, 'tied': 29} -> 0.03%


board size: 2x2
MCTSPolicy(num_playouts=5000, reset_tree=True) vs MCTSPolicy(num_playouts=10000, reset_tree=True):
MCTSPolicy(num_playouts=10000, reset_tree=True): {'lost': 46, 'won': 49, 'tied': 5} -> 49%

board size: 2x2
MCTSPolicy2(num_playouts=5000) vs MCTSPolicy2(num_playouts=10000):
MCTSPolicy2(num_playouts=10000): MCTSPolicy2: {'tied': 24, 'won': 37, 'lost': 39} -> 37%


----------------------------------------------------------------------------------------------
2x2                                                     |  L0        |  L1        |  L2
----------------------------------------------------------------------------------------------
MCTSPolicy (500, F)                                     |  80% / -   |  23% / -   |
MCTSPolicy (1500, F)                                    |  82% / -   |  27% / -   |
MCTSPolicy (500, T)                                     |  83% / -   |  33% / -   |
MCTSPolicy (1500, T)                                    |  87% / -   |  43% / -   |
MCTSPolicy (3000, T)                                    |            |  48% / -   |
MCTSPolicy (5000, T)                                    |  90% / -   |            |
MCTSPolicy (10000,T)                                    |            |  53% / -   |
----------------------------------------------------------------------------------------------
MCTSPolicy2 (100)                                       |  87% / 82% |  20% / 24% |   4% / 11%
MCTSPolicy2 (500)                                       |  90% / 89% |  32% / 33% |  18% / 17%
MCTSPolicy2 (1000)                                      |  88% / 87% |  30% / 43% |  28% / 23%
MCTSPolicy2 (5000)                                      |  96% / -   |  57% / -   |  27% / -
MCTSPolicy2 (10000)                                     |            |  64% / -   |  31% / -
MCTSPolicy2 (50000)                                     |            |  65% / -   |  38% / -
MCTSPolicy2 (100000)                                    |            |            |
----------------------------------------------------------------------------------------------
MCTSRootParallelPolicy (1000, 4)                        |  92% / -   |  34% / -   |  25% / -
MCTSRootParallelPolicy (1000, 8)                        |            |  41% / -   |
----------------------------------------------------------------------------------------------
MCTSPolicy2 (250, Level2HeuristicPolicy)                |  92% / -   |  54% / -   |  55% / -
MCTSPolicy2 (1000, Level2HeuristicPolicy)               |  96% / -   |  62% / -   |  51% / -
MCTSPolicy2 (50000, Level2HeuristicPolicy)              |            |  80% / -   |
----------------------------------------------------------------------------------------------
MCTSPolicy2 (1000, PGPolicyCNN2(                        |            |            |
 pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)) |            |  55% / -   |
----------------------------------------------------------------------------------------------
MCTSRootParallelPolicy (250, 4, Level2HeuristicPolicy)  |  91% / -   |  59% / -   |  64% / -
MCTSRootParallelPolicy (1000, 4, Level2HeuristicPolicy) |  90% / -   |  57% / -   |  59% / -
MCTSRootParallelPolicy (250, 8, Level2HeuristicPolicy)  |  92% / -   |  55% / -   |  67% / -
----------------------------------------------------------------------------------------------
* after 100 games, alternating going first
* MCTSPolicy2 ([num simulations])
* MCTSPolicy ([num simulations], [reset tree: {T}rue or {F}alse])
* MCTSRootParallelPolicy ([num simulations], [num workers])
* MCTSPolicy2 ([num simulations], [default policy])
* MCTS selection policy: [most visited node] / [highest average wins]


--------------------------------------------------------------------------------------------------------------------------------------
2x2        |  L0     |  L1     |  L2     |  PG    |  MCTS-L2  |  MCTS  |  MCTS-PG  |  N-MCTS-L2  |  N-MCTS  |  C-MCTS  |  C-MCTS-L2  |
--------------------------------------------------------------------------------------------------------------------------------------
L0         |  -      |  4.9%   |  2.8%   |  2.9%  |  3.1%     |  2.4%  |  2.4%     |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
L1         |  76.3%  |  -      |  13.1%  |  12.4% |  26.1%    |  27.3% |  28.1%    |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
L2         |  78.2%  |  51.8%  |  -      |  21.9% |  31.1%    |  42.5% |  36.5%    |  22.1%      |  34.0%   |  22.0%   |  18.9%      |
--------------------------------------------------------------------------------------------------------------------------------------
PG         |  89.7%  |  67.7%  |  68.7%  |  -     |  61.9%    |  57.6% |  50%^     |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
MCTS-L2    |  92.6%  |  62.2%  |  56.8%  |  33.4% |  -        |        |           |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
MCTS       |  90.2%  |  38.6%  |  21.9%  |  7.2%  |           |  -     |           |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
MCTS-PG    |  92.8%  |  60.8%  |  53.5%  |  50%^  |           |        |  -        |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
N-MCTS-L2  |         |         |  60.1%  |        |           |        |           |  -          |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
N-MCTS     |         |         |  19.7%  |        |           |        |           |             |  -       |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
C-MCTS     |         |         |  41.4%  |        |           |        |           |             |          |  -       |             |
--------------------------------------------------------------------------------------------------------------------------------------
C-MCTS-L2  |         |         |  71.6%  |        |           |        |           |             |          |          |  -          |
--------------------------------------------------------------------------------------------------------------------------------------
* after 1000, games, alternating going first
* PG: PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* MCTS-L2: MCTSPolicy2(1000, default_policy=Level2HeuristicPolicy)
* MCTS: MCTSPolicy2(1000)
* MCTS-PG: MCTSPolicy2(1000, default_policy=PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt))
* N-MCTS-L2: MCTSPolicyNetPolicy(board_size, num_playouts=1000, w=10, default_policy=Level2HeuristicPolicy(board_size))
             with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* N-MCTS: MCTSPolicyNetPolicy(board_size, num_playouts=1000, w=10)
          with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* C-MCTS: MCTSPolicyNetPolicyCpuct(board_size, num_playouts=1000, cpuct=5)
          with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* C-MCTS-L2: MCTSPolicyNetPolicyCpuct(board_size, num_playouts=1000, cpuct=5, default_policy=Level2HeuristicPolicy(board_size))
             with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
^ after 30 games; gameplay is too deterministic


----------------------------------------------------------------------
3x3                             |  L0        |  L1        |  L2
----------------------------------------------------------------------
MCTSPolicy2 (100)               |  90% / 90% |   7% / 7%  |   3% / 8%
MCTSPolicy2 (500)               | 100% / 99% |  13% / 19% |  11% / 16%
MCTSPolicy2 (1000)              |            |            |
MCTSPolicy2 (5000)              |            |            |
MCTSPolicy2 (10000)             |            |            |  40% / -
MCTSPolicy2 (50000)             |            |  74% / -   |  54% / -
MCTSPolicy2 (100000)            |            |  82% / -   |
----------------------------------------------------------------------
MCTSRootParallelPolicy (100, 4) |  96% / -   |   5% / -   |
----------------------------------------------------------------------
* after 100 games, alternating going first
* MCTSPolicy2 ([num simulations])
* MCTSRootParallelPolicy ([num simulations], [num workers])
* MCTS selection policy: [most visited node] / [highest average wins]

------------------------------------------------------
5x5                 |  L0      |  L1      |  L2
------------------------------------------------------
MCTSPolicy2 (100)   |  93% / - |   0% / - |
MCTSPolicy2 (500)   |          |          |
MCTSPolicy2 (1000)  |          |          |
MCTSPolicy2 (5000)  |          |          |
MCTSPolicy2 (10000) |          |          |
MCTSPolicy2 (50000) |          |          |
MCTSPolicy2 (100000)|          |          |
------------------------------------------------------
* after 100 games, alternating going first
* MCTSPolicy2 ([num simulations])
* MCTS selection policy: [most visited node] / [highest average wins]

===========================

MCTS+NN (# simulations in parentheses) win rate vs NN:
--------------------------------------------------------------------
3x3              |   NN                           |  L2            |
--------------------------------------------------------------------
MCTS+NN (100)    |  27% (367 games, ~6 sec./game) |                |
MCTS+NN (500)    |  48% (90 games, ~30 sec./game) |                |
MCTS+NN (1000)   |  58% (90 games, ~55 sec./game) | 44% (50 games) |
MCTS+NN (1500)   |  62% (65 games, ~85 sec./game) |                |
MCTS+NN (2500)   |  59% (90 games, ~130 sec./game)|                |
MCTS+NN (5000)   |  80% (46 games, ~300 sec./game)| 58% (12 games) |
--------------------------------------------------------------------
NN = PGPolicy3x3CNN(dnbpy38-3x3-relu-351000.txt, activation=tf.nn.relu)
MCTS+NN = MCTSPolicy2(default_policy=NN)

TODO have duels between L2 and MCTS+NN and NN
duel: MCTS+NN (100 simulations) vs. NN: NN wins 33-15 (~31% win rate for MCTS+NN)
duel: MCTS+NN (500 simulations) vs. NN: NN wins 28-20 (~42% win rate for MCTS+NN)
duel: L2 vs. NN: NN wins 42-6 (87.5% win rate for NN)

MCTS+NN (1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 1-8 [loss]
policy goes second: 5-4 [win]

MCTS+NN (1500 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 3-6 [loss]

MCTS+NN (2500 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 9-0 [loss]
policy goes second: 4-5 [loss]

MCTS+NN (5000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 1-8 [loss]
policy goes second: 2-7 [loss]


N-MCTS (w, # simulations in parentheses) win rate vs NN:
-----------------------------------------------------------------------
3x3                 |   NN                           |  L2            |
-----------------------------------------------------------------------
N-MCTS (1, 1000)    |  12% (77 games, ~12 sec./game) | 24% (75 games) |
N-MCTS (10, 1000)   |  24% (82 games, ~12 sec./game) | 47% (121 games)|
N-MCTS (50, 1000)   |  54% (100 games, ~10 sec./game)| 79% (100 games)|
N-MCTS (100, 1000)  |  70% (100 games, ~12 sec./game)| 88% (100 games)|
N-MCTS (500, 1000)  |  65% (124 games, ~15 sec/game) | 87% (116 games)|
-----------------------------------------------------------------------
N-MCTS (100, 5000)  |  54% (100 games, ~70 sec./game)| 83% (100 games)|
N-MCTS (500, 5000)  |  55% (100 games, ~77 sec./game)| 92% (100 games)|
N-MCTS (1000, 5000) |  88% (100 games, ~70 sec./game)| 92% (100 games)|
-----------------------------------------------------------------------
N-MCTS[N] (100, 1000) |  96% (100 games, ~72 sec/game) | 87% (100 games)|
-----------------------------------------------------------------------
N-MCTS[L] (100, 1000) |  63% (100 games, ~42 sec/game) | 76% (100 games)|
-----------------------------------------------------------------------
MCTS[N] (1000)      |  56% (393 games, ~60 sec/game) | 48% (390 games)|
-----------------------------------------------------------------------
NN = PGPolicy3x3CNN(dnbpy38-3x3-relu-351000.txt, activation=tf.nn.relu)
N-MCTS = MCTSPolicyNetPolicy, with policy = NN
N-MCTS[N] = MCTSPolicyNetPolicy, with policy = NN, with default policy NN
N-MCTS[L] = MCTSPolicyNetPolicy, with policy = NN, with default policy L2
MCTS[N] = MCTSPolicy2 with default policy NN

duel: N-MCTS (w=100, n=1000) vs. NN: N-MCTS wins 32-16 (~67% win rate for N-MCTS)
duel: N-MCTS (w=120, n=1000) vs. NN: N-MCTS wins 27-21 (~56% win rate for N-MCTS)
duel: N-MCTS (w=500, n=5000) vs. NN: N-MCTS wins 28-20 (~58% win rate for N-MCTS)
duel: N-MCTS (w=1000, n=5000) vs. NN: N-MCTS wins 36-12 (75% win rate for N-MCTS)
duel: N-MCTS (w=100, n=1000) vs. NN: NN wins 35-13 (~27% win rate for N-MCTS) *normalize_with_softmax=True
duel: N-MCTS[N] (w=100, n=1000) vs. NN: N-MCTS[N] wins 43-5 (~90% win rate for N-MCTS[N])
duel: N-MCTS[N] (w=100, n=1000) vs. L2: N-MCTS[N] wins 42-6 (~88% win rate for N-MCTS[N])
duel: N-MCTS[L] (w=100, n=1000) vs. L2: N-MCTS[L] wins 39-9 (~81% win rate for N-MCTS[L])
duel: N-MCTS[L] (w=100, n=1000) vs. NN: N-MCTS[L] wins 29-19 (~60% win rate for N-MCTS[L])
duel: MCTS[N] (n=1000) vs. NN: MCTS[N] wins 28-20 (~58% win rate for MCTS[N])
duel: MCTS[N] (n=1000) vs. L2: L2 wins 27-21 (~44% win rate for MCTS[N])

N-MCTS (w=1000, 5000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 4-5 [loss]

N-MCTS (w=100, 1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 6-3 [win]

N-MCTS[N] (w=100, 1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 7-2 [win]
policy goes first: 1-8 [loss]
policy goes second: 7-2 [win]
policy goes first: 1-8 [loss]

N-MCTS[L] (w=100, 1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 2-7 [loss]

N-MCTS[N] (w=1000, 5000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 3-6 [loss]

N2-MCTS[N2] (w=100, 1000 simulations) vs. dotsandboxes.org 3x3:
* N2 = PGPolicy3x3CNN(dnbpy43-3x3-tanh-305000.txt, activation=tf.nn.tanh)
policy goes first: 5-4 [win] (performed a double-deal move)
policy goes second: 4-5 [loss]

N2-MCTS[N2] (w=1000, 5000 simulations) vs. dotsandboxes.org 3x3:
* N2 = PGPolicy3x3CNN(dnbpy43-3x3-tanh-305000.txt, activation=tf.nn.tanh)
policy goes first: 3-6 [loss]
policy goes second: 3-6 [loss]

=============

dnbpy44 @ 62000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy44/search-weights-62000.txt, activation=tf.nn.tanh)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy44/weights-62000.txt, activation=tf.nn.tanh)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.4% (L0), 81.3% (L1), 60.5% (L2)

tournaments (1000 games):
-------------------------
3x3     |  L1   |  L2   |
-------------------------
SP      | 82.0% | 61.3% |
-------------------------
IP      | 79.3% | 58.0% |
-------------------------
MCTS+SP | 85.0% | 67.0% |
-------------------------

duels:
---------------------------------------------------------------------
3x3     |   SP   |   IP   | MCTS+SP |       L2 (run 1, 2, 3)        |
---------------------------------------------------------------------
SP      |   -    |   52%  |         |   63%, 77%, 60% (avg. 66.7%)  |
---------------------------------------------------------------------
IP      |   48%  |   -    |   52%   |   48%, 50%, 52% (avg. 50.0%)  |
---------------------------------------------------------------------
MCTS+SP |        |   48%  |   -     |   65%, 69%, 58%, (avg. 64.0%) | +14.0%
---------------------------------------------------------------------
L2      |   37%  |   52%  |   35%   |   -                           |
---------------------------------------------------------------------

dnbpy44 @ 152000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy44/search-weights-152000.txt, activation=tf.nn.tanh)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy44/weights-152000.txt, activation=tf.nn.tanh)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.6% (L0), 86.9% (L1), 68.3% (L2)

tournaments (1000 games):
-------------------------
3x3     |  L1   |  L2   |
-------------------------
SP      | 87.8% | 65.3% |
-------------------------
IP      | 84.9% | 63.6% |
-------------------------
MCTS+SP | 88.2% | 76.4% |
-------------------------

duels:
---------------------------------------------------------------------
3x3     |   SP   |   IP   | MCTS+SP |       L2 (run 1, 2, 3)        |
---------------------------------------------------------------------
SP      |   -    |   23%  |         |   63%  52%, 56% (avg. 57.0%)  |
---------------------------------------------------------------------
IP      |   77%  |   -    |   69%   |   63%, 75%, 60% (avg. 66.0%)  |
---------------------------------------------------------------------
MCTS+SP |        |   31%  |   -     |   77%, 75%, 67% (avg. 73.0%)  | +7.0%
---------------------------------------------------------------------
L2      |   37%  |   37%  |   23%   |   -                           |
---------------------------------------------------------------------

dnbpy44 @ 305000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy44/search-weights-305000.txt, activation=tf.nn.tanh)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy44/weights-305000.txt, activation=tf.nn.tanh)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
MCTS+SP2 (MCTS with search policy and search policy as default policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)

- IP: 99.7% (L0), 92.0% (L1), 80.1% (L2)

tournaments (1000 games):
-------------------------------------------------------
3x3     |  L1   |  L2 run 1  |  L2 run 2  | L2 run 3  |
-------------------------------------------------------
SP      | 91.7% |  77.4%     |  78.4%     | 75.8%     |
-------------------------------------------------------
IP      | 91.2% |  79.5%     |  78.5%     | 80.5%     |
-------------------------------------------------------
MCTS+SP | 92.7% |  84.3%     |  86.5%     |           |  (~9 sec/game)
-------------------------------------------------------
MCTS+SP2| 91.4% (64 games)  |  88.7% (53 games, ~60 sec/game) |
-------------------------------------------------------

duels:
---------------------------------------------------------------------
3x3     |   SP   |   IP   | MCTS+SP |       L2 (run 1, 2, 3)        |
---------------------------------------------------------------------
SP      |   -    |   27%  |         |   73%  79%, 73% (avg. 75.0%)  |
---------------------------------------------------------------------
IP      |   73%  |   -    |   58%   |   79%, 85%, 81% (avg. 81.7%)  |
---------------------------------------------------------------------
MCTS+SP |        |   42%  |   -     |   85%, 88%, 85% (avg. 86.0%)  | +4.3%
---------------------------------------------------------------------
L2      |   27%  |   21%  |   15%   |   -                           |
---------------------------------------------------------------------

MCTS+SP2 (c=5, 1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 4-5 [loss]
policy goes second: 2-7 [loss]

=============

dnbpy46 @ 209000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy46/search-weights-209000.txt, activation=tf.nn.tanh)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy46/weights-209000.txt, activation=tf.nn.tanh)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 95.4% (L1), 88.3% (L2)

duels:
---------------------------------------------------------------------
3x3     |   SP   |   IP   | MCTS+SP |       L2 (run 1, 2, 3)        |
---------------------------------------------------------------------
SP      |   -    |   31%  |         |   88%  85%, 92% (avg. 88.3%)  |
---------------------------------------------------------------------
IP      |   69%  |   -    |   52%   |   88%, 83%, 83% (avg. 84.7%)  |
---------------------------------------------------------------------
MCTS+SP |        |   48%  |   -     |   88%, 90%, 94% (avg. 90.7%)  |
---------------------------------------------------------------------
L2      |   12%  |   12%  |   12%   |   -                           |
---------------------------------------------------------------------

MCTS+SP (n=3000, c=5):
duel vs. L2: 93.8%

MCTS+SP (n=5000, c=5):
duel vs. L2: 100.0%

MCTS+SP+SP-rollout (n=1000, c=5):
duel vs. L2: 91.6%

MCTS+SP+IP-rollout (n=1000, c=5):
duel vs. L2: 91.6%

MCTS_SP+SP-sampling-rollout (n=1000, c=5):
duel vs. L2: 87.5%

N-MCTS+SP+SP-rollout (n=1000, w=100):
duel vs. L2: 85.4%

SP vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 2-7 [loss]

IP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 3-6 [loss]

MCTS+SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 4-5 [loss]

MCTS+SP+SP-rollout vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 5-4 [win] (performed a double-deal move)
policy goes second: 4-5 [loss]

=============

dnbpy48 @ 228000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy48/search-weights-228000.txt, activation=tf.nn.relu)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy48/weights-228000.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 95.5% (L1), 91.0% (L2)

duels:
---------------------------------------------------------------------
3x3     |   SP   |   IP   | MCTS+SP |       L2 (run 1, 2, 3)        |
---------------------------------------------------------------------
SP      |   -    |   48%  |         |   95.5%  87.5%, 83.3% (avg. 88.8%)  |
---------------------------------------------------------------------
IP      |   52%  |   -    |   %   |   85.4%, 95.8%, 91.7% (avg. 91.0%)  |
---------------------------------------------------------------------
MCTS+SP |        |   %  |   -     |   79.2%, %, % (avg. %)  |
---------------------------------------------------------------------
MCTS+IP |        |   60.4%  |   -     |   97.9%, 97.9%, 87.5% (avg. 94.4%)  |
---------------------------------------------------------------------

MCTS+SP (n=1000, c=5) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 2-7 [loss]

MCTS+IP (n=1000, c=5) vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 6-3 [win]
policy goes second: 3-6 [loss]

MCTS+IP+IP-rollout (n=1000, c=5) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 3-6 [loss]

MCTS+IP (n=5000, c=5) vs. dotsandboxes.org 3x3:
policy goes second: 2-7 [loss]

IP vs. dotsandboxes.org 3x3:
policy goes second: 2-7 [loss]

dnbpy48 @ 292000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy48/search-weights-292000.txt, activation=tf.nn.relu)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy48/weights-292000.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 97.6% (L1), 91.0% (L2)

duel IP vs IP@228000:
IP wins 27-21 (56.3%)

duel IP vs MCTS+IP:
MCTS+IP wins 30-18 (62.5%)

duel IP vs MCTS+SP:
IP wins 26-22 (54.2%)

IP vs. dotsandboxes.org 3x3:
policy goes second: 3-6 [loss]

MCTS+IP vs. dotsandboxes.org 3x3:
policy goes second: 2-7 [loss]

MCTS+IP(n=10000, c=5) vs. dotsandboxes.org 3x3:
policy goes second: 2-7 [loss]

MCTS+SP(n=10000, c=5) vs. dotsandboxes.org 3x3:
policy goes second: 3-6 [loss]

dnbpy48 @ 433000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy48/search-weights-433000.txt, activation=tf.nn.relu)
IP (imitation policy):  PGPolicy3x3CNN(dnbpy48/weights-433000.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.9% (L0), 97.4% (L1), 93.5% (L2)

duel IP vs IP@292000:
IP wins 28-20 (58.3%)

duel IP vs MCTS+IP:
MCTS-IP wins 25-23 (52.1%)

duel IP vs MCTS+SP:
IP wins 26-22 (54.2%)

duel MCTS+IP vs L2:
MCTS+IP wins 45-3 (93.8%)

duel MCTS+SP vs L2:
MCTS+SP wins 43-5 (89.6%)

duel IP vs L2:
IP wins 45-3 (93.8%), 41-7, 45-3, 43-5, 41-7

duel SP vs L2:
SP wins 46-2, 43-5, 42-6, 47-1, 43-5

duel SP vs IP:
IP wins 30-18 (62.5%)

IP vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 3-6 [loss]

MCTS+IP vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 3-6 [loss]

MCTS+SP vs. dotsandboxes.org 3x3:
policy goes second: 3-6 [loss]

- performed a double-deal against human player

SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 6-3 [win] (website starts at edge 10)

SP vs. Dabble 3x3:
policy goes first: 2-7 [loss]
policy goes second: 4-5 [loss]

MCTS+SP vs. Dabble 3x3:
policy goes first: 2-7 [loss]
policy goes second: 4-5 [loss]

IP vs. Dabble 3x3:
policy goes first: 2-7 [loss]
policy goes second: 4-5 [loss]

SP vs. PRsBoxes 3x3:
policy goes first: 4-5 [loss]
policy goes second: 6-3 [win]

MCTS+SP vs. PRsBoxes 3x3:
policy goes first: 4-5 [loss]
policy goes second: 5-4 [win]

IP vs. PRsBoxes 3x3:
policy goes first: 1-8 [loss]
policy goes second: 5-4 [win]

SP vs. L2 (10,000 games): 91.42%
IP vs. L2 (10,000 games): 91.26%
MCTS+SP vs. L2 (10,000 games): 91.18%

=============

dnbpy52 @ 225000:
----------------

SP (search policy): PGPolicy3x3CNN(dnbpy52/weights-225000.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.9% (L0), 97.4% (L1), 93.0% (L2)

SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 2-7 [loss]

SP vs. L2 (100 games): 93%, 90%, 91%, 91%, 88%, 95%, 89%, 91%, 89%, 90% (mean: 90.7, std: 1.952)
SP vs. L2 (1,000 games): 90.2%, 89.7%, 91.3%, 89.9%, 90.0%, 90.8%, 91.1%, 89.5%, 88.0%, 89.3% (mean: 89.98, std: 0.917)
SP vs. L2 (10,000 games): 90.06%, 90.30% (on chilidog), 90.44% (mean: 90.27, std: 0.157)
SP@287000[L0:100.0%, L1:97.1%, L2:92.9%] vs. L2 (10,000 games): 90.05% (on chilidog)
SP@303000[L0:100.0%, L1:96.7%, L2:91.6%] vs. L2 (10,000 games): 89.3% (on chilidog)
* the discrepancy between evaluation performance during training and post-training does not seem to be due to noise
  - the performance on 1,000 games seems to vary only +/- 1% from the performance on 10,000 games, but we see a
    difference of more than 2%, consistently
  - the difference doesn't seem to be due to using the evaluator.py evaluate function
SP vs. L0 (10,000 games): 99.87%
SP vs. L1 (10,000 games): 96.79%

==============

dnbpy53 @ 205:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy53/weights-205.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 97.1% (L1), 94.2% (L2)

SP vs. dotsandboxes.org 3x3:
policy goes first: 5-4 [win]
policy goes second: 1-8 [loss] (website starts at edge 0)
policy goes second: 2-7 [loss] (website starts at edge 10)

MCTS+SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 3-6 [loss]

SP vs. L2 (10,000 games): 93.12%

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP@dnbpy48/search-weights-433000.txt wins 27-21

SP vs. IP@dnbpy48/weights-433000.txt (duel):
IP@dnbpy48/weights-433000.txt wins 27-21

==============

dnbpy54 @ 244:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy54/weights-244.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.9% (L0), 96.4% (L1), 93.1% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP@dnbpy48/search-weights-433000.txt wins 25-23

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP@dnbpy54/weights-262.txt wins 26-22

SP vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 6-3 [win] (website starts at edge 10)

dnbpy54 @ 262:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy54/weights-262.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 97.0% (L1), 94.2% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP wins 27-21

SP vs. dotsandboxes.org 3x3:
policy goes first: 5-4 [win] (performs double deal)
policy goes second: 4-5 [loss] (website starts at edge 0)

dnbpy54 @ 309:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy54/weights-309.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.8% (L0), 97.1% (L1), 94.2% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP wins 27-21

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP wins 28-20

SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 2-7 [loss]

dnbpy54 @ 323:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy54/weights-323.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 98.5% (L1), 93.9% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP@dnbpy48/search-weights-433000.txt wins 28-20

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP@dnbpy54/weights-262.txt wins 26-22

SP vs. SP@dnbpy54/weights-309.txt (duel):
SP@dnbpy54/weights-309.txt wins 25-23

SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 2-7 [loss]

dnbpy54 @ 460
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy54/weights-460.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 96.8% (L1), 94.8% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP wins 28-20

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP wins 28-20

SP vs. SP@dnbpy54/weights-309.txt (duel):
SP wins 31-17

SP vs. SP@dnbpy54/weights-244.txt (duel):
SP@dnbpy54/weights-244.txt wins 34-14

SP vs. SP@dnbpy54/weights-323.txt (duel):
SP@dnbpy54/weights-323.txt wins 31-17

SP vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 3-6 [loss]

==============

dnbpy55 @ 63:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy55/weights-63.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 97.4% (L1), 92.1% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP wins 30-18

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP@dnbpy54/weights-262.txt wins 27-21

SP vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 6-3 [win]

dnbpy55 @ 135:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy55/weights-135.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 100.0% (L0), 97.7% (L1), 94.6% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
SP wins 34-14

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP@dnbpy54/weights-262.txt wins 27-21

SP vs. SP@dnbpy55/weights-63.txt (duel):
SP wins 34-14

SP vs. dotsandboxes.org 3x3:
policy goes first: 1-8 [loss]
policy goes second: 3-6 [loss]

==============

dnbpy57 @ 139:
----------------

SP (search policy): PGPolicy3x3CNN5Layer(dnbpy57/weights-139.txt, activation=tf.nn.relu)
MCTS+SP (MCTS with search policy) : MCTSPolicyNetPolicyCpuct(num_playouts=1000, cpuct=5)
- IP: 99.9% (L0), 96.1% (L1), 92.7% (L2)

SP vs. SP@dnbpy48/search-weights-433000.txt (duel):
tie 24-24

SP vs. SP@dnbpy54/weights-262.txt (duel):
SP@dnbpy54/weights-262.txt wins 28-20

SP vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 4-5 [loss]  (website starts at edge 0)
policy goes second: 3-6 [loss]  (website starts at edge 10)
