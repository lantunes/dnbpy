Results:

board size: 2x2
player 1 (RandomPolicy) vs. player 2 (Level1HeuristicPolicy)
RandomPolicy: {'lost': 7761, 'tied': 1799, 'won': 440}; Level1HeuristicPolicy: {'lost': 440, 'tied': 1799, 'won': 7761}

board size: 2x2
player 1 (RandomPolicy) vs. player 2 (Level2HeuristicPolicy)
RandomPolicy: {'won': 312, 'tied': 1693, 'lost': 7995}; Level2HeuristicPolicy: {'won': 7995, 'tied': 1693, 'lost': 312}

board size: 2x2
player 1 (Level1HeuristicPolicy) vs. player 2 (Level2HeuristicPolicy)
Level1HeuristicPolicy: {'lost': 5408, 'tied': 3361, 'won': 1231}; Level2HeuristicPolicy: {'lost': 1231, 'tied': 3361, 'won': 5408}

board size: 3x3
player 1 (RandomPolicy) vs. player 2 (Level1HeuristicPolicy)
RandomPolicy: {'tied': 0, 'won': 31, 'lost': 9969}; Level1HeuristicPolicy: {'tied': 0, 'won': 9969, 'lost': 31}

board size: 3x3
player 1 (RandomPolicy) vs. player 2 (Level2HeuristicPolicy)
RandomPolicy: {'lost': 9966, 'won': 34, 'tied': 0}; Level2HeuristicPolicy: {'lost': 34, 'won': 9966, 'tied': 0}

board size: 3x3
player 1 (Level1HeuristicPolicy) vs. player 2 (Level2HeuristicPolicy)
Level1HeuristicPolicy: {'lost': 8409, 'won': 1591, 'tied': 0}; Level2HeuristicPolicy: {'lost': 1591, 'won': 8409, 'tied': 0}

board size: 5x5
player 1 (RandomPolicy) vs. player 2 (Level1HeuristicPolicy)
RandomPolicy: {'lost': 10000, 'won': 0, 'tied': 0}; Level1HeuristicPolicy: {'lost': 0, 'won': 10000, 'tied': 0}

board size: 5x5
player 1 (RandomPolicy) vs. player 2 (Level2HeuristicPolicy)
RandomPolicy: {'lost': 10000, 'tied': 0, 'won': 0}; Level2HeuristicPolicy: {'lost': 0, 'tied': 0, 'won': 10000}

board size: 5x5
player 1 (Level1HeuristicPolicy) vs. player 2 (Level2HeuristicPolicy)
Level1HeuristicPolicy: {'tied': 0, 'won': 461, 'lost': 9539}; Level2HeuristicPolicy: {'tied': 0, 'won': 9539, 'lost': 461}


    ----------------------------------------------------------------------------------------
            | L0 2x2 | L1 2x2 | L2 2x2 | L0 3x3 | L1 3x3 | L2 3x3 | L0 5x5 | L1 5x5 | L2 5x5
    ----------------------------------------------------------------------------------------
     L0 2x2 |   -    |  4.40% |  3.12% |   -    |   -    |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L1 2x2 | 77.61% |    -   | 12.31% |   -    |   -    |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L2 2x2 | 79.95% | 54.08% |    -   |   -    |   -    |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L0 3x3 |   -    |    -   |    -   |   -    | 0.31%  | 0.34%  |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L1 3x3 |   -    |    -   |    -   | 99.69% |   -    | 15.91% |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L2 3x3 |   -    |    -   |    -   | 99.66% | 84.09% |   -    |    -   |   -    |   -
    ----------------------------------------------------------------------------------------
     L0 5x5 |   -    |    -   |    -   |   -    |   -    |   -    |    -   |   0%   |   0%
    ----------------------------------------------------------------------------------------
     L1 5x5 |   -    |    -   |    -   |   -    |   -    |   -    |  100%  |   -    | 4.61%
    ----------------------------------------------------------------------------------------
     L2 5x5 |   -    |    -   |    -   |   -    |   -    |   -    |  100%  | 95.39% |   -
    ----------------------------------------------------------------------------------------
    Win rates of L0, L1, and L2 policies versus each other on 2x2, 3x3, and 5x5,
     after tournaments consisting of 10,000 games each.
    L0: A policy that selects an edge at random.
    L1: A policy that selects an edge randomly that completes a box, if possible. Otherwise,
        it selects an edge at random.
    L2: A policy that selects an edge randomly that completes a box, if possible. Otherwise,
        it selects an edge randomly that does not allow the opponent to complete a box.
        Otherwise, it selects an edge at random.

board size: 2x2
CausalEntropicPolicy(max_sample_paths=1000) vs RandomPolicy:
CausalEntropicPolicy: {'won': 364, 'lost': 21, 'tied': 115} -> 72.8%
CausalEntropicPolicy(max_sample_paths=10000) vs RandomPolicy:
CausalEntropicPolicy: {'won': 69, 'lost': 1, 'tied': 30} -> 69%

board size: 2x2
CausalEntropicPolicy(max_sample_paths=1000) vs Level1HeuristicPolicy:
CausalEntropicPolicy: {'won': 5, 'lost': 371, 'tied': 124} -> 0.01%
CausalEntropicPolicy(max_sample_paths=10000) vs Level1HeuristicPolicy:
CausalEntropicPolicy: {'won': 4, 'lost': 84, 'tied': 29} -> 0.03%


board size: 2x2
MCTSPolicy(num_playouts=5000, reset_tree=True) vs MCTSPolicy(num_playouts=10000, reset_tree=True):
MCTSPolicy(num_playouts=10000, reset_tree=True): {'lost': 46, 'won': 49, 'tied': 5} -> 49%

board size: 2x2
MCTSPolicy2(num_playouts=5000) vs MCTSPolicy2(num_playouts=10000):
MCTSPolicy2(num_playouts=10000): MCTSPolicy2: {'tied': 24, 'won': 37, 'lost': 39} -> 37%


----------------------------------------------------------------------------------------------
2x2                                                     |  L0        |  L1        |  L2
----------------------------------------------------------------------------------------------
MCTSPolicy (500, F)                                     |  80% / -   |  23% / -   |
MCTSPolicy (1500, F)                                    |  82% / -   |  27% / -   |
MCTSPolicy (500, T)                                     |  83% / -   |  33% / -   |
MCTSPolicy (1500, T)                                    |  87% / -   |  43% / -   |
MCTSPolicy (3000, T)                                    |            |  48% / -   |
MCTSPolicy (5000, T)                                    |  90% / -   |            |
MCTSPolicy (10000,T)                                    |            |  53% / -   |
----------------------------------------------------------------------------------------------
MCTSPolicy2 (100)                                       |  87% / 82% |  20% / 24% |   4% / 11%
MCTSPolicy2 (500)                                       |  90% / 89% |  32% / 33% |  18% / 17%
MCTSPolicy2 (1000)                                      |  88% / 87% |  30% / 43% |  28% / 23%
MCTSPolicy2 (5000)                                      |  96% / -   |  57% / -   |  27% / -
MCTSPolicy2 (10000)                                     |            |  64% / -   |  31% / -
MCTSPolicy2 (50000)                                     |            |  65% / -   |  38% / -
MCTSPolicy2 (100000)                                    |            |            |
----------------------------------------------------------------------------------------------
MCTSRootParallelPolicy (1000, 4)                        |  92% / -   |  34% / -   |  25% / -
MCTSRootParallelPolicy (1000, 8)                        |            |  41% / -   |
----------------------------------------------------------------------------------------------
MCTSPolicy2 (250, Level2HeuristicPolicy)                |  92% / -   |  54% / -   |  55% / -
MCTSPolicy2 (1000, Level2HeuristicPolicy)               |  96% / -   |  62% / -   |  51% / -
MCTSPolicy2 (50000, Level2HeuristicPolicy)              |            |  80% / -   |
----------------------------------------------------------------------------------------------
MCTSPolicy2 (1000, PGPolicyCNN2(                        |            |            |
 pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)) |            |  55% / -   |
----------------------------------------------------------------------------------------------
MCTSRootParallelPolicy (250, 4, Level2HeuristicPolicy)  |  91% / -   |  59% / -   |  64% / -
MCTSRootParallelPolicy (1000, 4, Level2HeuristicPolicy) |  90% / -   |  57% / -   |  59% / -
MCTSRootParallelPolicy (250, 8, Level2HeuristicPolicy)  |  92% / -   |  55% / -   |  67% / -
----------------------------------------------------------------------------------------------
* after 100 games, alternating going first
* MCTSPolicy2 ([num simulations])
* MCTSPolicy ([num simulations], [reset tree: {T}rue or {F}alse])
* MCTSRootParallelPolicy ([num simulations], [num workers])
* MCTSPolicy2 ([num simulations], [default policy])
* MCTS selection policy: [most visited node] / [highest average wins]


--------------------------------------------------------------------------------------------------------------------------------------
2x2        |  L0     |  L1     |  L2     |  PG    |  MCTS-L2  |  MCTS  |  MCTS-PG  |  N-MCTS-L2  |  N-MCTS  |  C-MCTS  |  C-MCTS-L2  |
--------------------------------------------------------------------------------------------------------------------------------------
L0         |  -      |  4.9%   |  2.8%   |  2.9%  |  3.1%     |  2.4%  |  2.4%     |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
L1         |  76.3%  |  -      |  13.1%  |  12.4% |  26.1%    |  27.3% |  28.1%    |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
L2         |  78.2%  |  51.8%  |  -      |  21.9% |  31.1%    |  42.5% |  36.5%    |  22.1%      |  34.0%   |  22.0%   |  18.9%      |
--------------------------------------------------------------------------------------------------------------------------------------
PG         |  89.7%  |  67.7%  |  68.7%  |  -     |  61.9%    |  57.6% |  50%^     |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
MCTS-L2    |  92.6%  |  62.2%  |  56.8%  |  33.4% |  -        |        |           |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
MCTS       |  90.2%  |  38.6%  |  21.9%  |  7.2%  |           |  -     |           |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
MCTS-PG    |  92.8%  |  60.8%  |  53.5%  |  50%^  |           |        |  -        |             |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
N-MCTS-L2  |         |         |  60.1%  |        |           |        |           |  -          |          |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
N-MCTS     |         |         |  19.7%  |        |           |        |           |             |  -       |          |             |
--------------------------------------------------------------------------------------------------------------------------------------
C-MCTS     |         |         |  41.4%  |        |           |        |           |             |          |  -       |             |
--------------------------------------------------------------------------------------------------------------------------------------
C-MCTS-L2  |         |         |  71.6%  |        |           |        |           |             |          |          |  -          |
--------------------------------------------------------------------------------------------------------------------------------------
* after 1000, games, alternating going first
* PG: PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* MCTS-L2: MCTSPolicy2(1000, default_policy=Level2HeuristicPolicy)
* MCTS: MCTSPolicy2(1000)
* MCTS-PG: MCTSPolicy2(1000, default_policy=PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt))
* N-MCTS-L2: MCTSPolicyNetPolicy(board_size, num_playouts=1000, w=10, default_policy=Level2HeuristicPolicy(board_size))
             with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* N-MCTS: MCTSPolicyNetPolicy(board_size, num_playouts=1000, w=10)
          with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* C-MCTS: MCTSPolicyNetPolicyCpuct(board_size, num_playouts=1000, cpuct=5)
          with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
* C-MCTS-L2: MCTSPolicyNetPolicyCpuct(board_size, num_playouts=1000, cpuct=5, default_policy=Level2HeuristicPolicy(board_size))
             with PGPolicyCNN2(pg_2x2_cnn2_vs_L0_L1_L2_batch_01-episode-1496000.txt)
^ after 30 games; gameplay is too deterministic


----------------------------------------------------------------------
3x3                             |  L0        |  L1        |  L2
----------------------------------------------------------------------
MCTSPolicy2 (100)               |  90% / 90% |   7% / 7%  |   3% / 8%
MCTSPolicy2 (500)               | 100% / 99% |  13% / 19% |  11% / 16%
MCTSPolicy2 (1000)              |            |            |
MCTSPolicy2 (5000)              |            |            |
MCTSPolicy2 (10000)             |            |            |  40% / -
MCTSPolicy2 (50000)             |            |  74% / -   |  54% / -
MCTSPolicy2 (100000)            |            |  82% / -   |
----------------------------------------------------------------------
MCTSRootParallelPolicy (100, 4) |  96% / -   |   5% / -   |
----------------------------------------------------------------------
* after 100 games, alternating going first
* MCTSPolicy2 ([num simulations])
* MCTSRootParallelPolicy ([num simulations], [num workers])
* MCTS selection policy: [most visited node] / [highest average wins]

------------------------------------------------------
5x5                 |  L0      |  L1      |  L2
------------------------------------------------------
MCTSPolicy2 (100)   |  93% / - |   0% / - |
MCTSPolicy2 (500)   |          |          |
MCTSPolicy2 (1000)  |          |          |
MCTSPolicy2 (5000)  |          |          |
MCTSPolicy2 (10000) |          |          |
MCTSPolicy2 (50000) |          |          |
MCTSPolicy2 (100000)|          |          |
------------------------------------------------------
* after 100 games, alternating going first
* MCTSPolicy2 ([num simulations])
* MCTS selection policy: [most visited node] / [highest average wins]

===========================

MCTS+NN (# simulations in parentheses) win rate vs NN:
--------------------------------------------------------------------
3x3              |   NN                           |  L2            |
--------------------------------------------------------------------
MCTS+NN (100)    |  27% (367 games, ~6 sec./game) |                |
MCTS+NN (500)    |  48% (90 games, ~30 sec./game) |                |
MCTS+NN (1000)   |  58% (90 games, ~55 sec./game) | 44% (50 games) |
MCTS+NN (1500)   |  62% (65 games, ~85 sec./game) |                |
MCTS+NN (2500)   |  59% (90 games, ~130 sec./game)|                |
MCTS+NN (5000)   |  80% (46 games, ~300 sec./game)| 58% (12 games) |
--------------------------------------------------------------------
NN = PGPolicy3x3CNN(dnbpy38-3x3-relu-351000.txt, activation=tf.nn.relu)
MCTS+NN = MCTSPolicy2(default_policy=NN)

TODO have duels between L2 and MCTS+NN and NN
duel: MCTS+NN (100 simulations) vs. NN: NN wins 33-15 (~31% win rate for MCTS+NN)
duel: MCTS+NN (500 simulations) vs. NN: NN wins 28-20 (~42% win rate for MCTS+NN)
duel: L2 vs. NN: NN wins 42-6 (87.5% win rate for NN)

MCTS+NN (1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 1-8 [loss]
policy goes second: 5-4 [win]

MCTS+NN (1500 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 3-6 [loss]

MCTS+NN (2500 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 9-0 [loss]
policy goes second: 4-5 [loss]

MCTS+NN (5000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 1-8 [loss]
policy goes second: 2-7 [loss]


MCTS-N (w, # simulations in parentheses) win rate vs NN:
-----------------------------------------------------------------------
3x3                 |   NN                           |  L2            |
-----------------------------------------------------------------------
MCTS-N (1, 1000)    |  12% (77 games, ~12 sec./game) | 24% (75 games) |
MCTS-N (10, 1000)   |  24% (82 games, ~12 sec./game) | 47% (121 games)|
MCTS-N (50, 1000)   |  54% (100 games, ~10 sec./game)| 79% (100 games)|
MCTS-N (100, 1000)  |  70% (100 games, ~12 sec./game)| 88% (100 games)|
MCTS-N (500, 1000)  |  65% (124 games, ~15 sec/game) | 87% (116 games)|
-----------------------------------------------------------------------
MCTS-N (100, 5000)  |  54% (100 games, ~70 sec./game)| 83% (100 games)|
MCTS-N (500, 5000)  |  55% (100 games, ~77 sec./game)| 92% (100 games)|
MCTS-N (1000, 5000) |  88% (100 games, ~70 sec./game)| 92% (100 games)|
-----------------------------------------------------------------------
NN = PGPolicy3x3CNN(dnbpy38-3x3-relu-351000.txt, activation=tf.nn.relu)
MCTS-N = MCTSPolicyNetPolicy, with policy = NN

duel: MCTS-N (w=100, n=1000) vs. NN: MCTS-N wins 32-16 (~67% win rate for MCTS-N)
duel: MCTS-N (w=120, n=1000) vs. NN: MCTS-N wins 27-21 (~56% win rate for MCTS-N)
duel: MCTS-N (w=500, n=5000) vs. NN: MCTS-N wins 28-20 (~58% win rate for MCTS-N)
duel: MCTS-N (w=1000, n=5000) vs. NN: MCTS-N wins 36-12 (75% win rate for MCTS-N)
duel: MCTS-N (w=100, n=1000) vs. NN: NN wins 35-13 (~27% win rate for MCTS-N) *normalize_with_softmax=True

MCTS-N (w=1000, 5000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 2-7 [loss]
policy goes second: 4-5 [loss]

MCTS-N (w=100, 1000 simulations) vs. dotsandboxes.org 3x3:
policy goes first: 3-6 [loss]
policy goes second: 6-3 [win]